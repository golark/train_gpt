# ğŸš€ GPT2 Performance Sub-$10

Welcome to the **GPT2-Beaters** repo! Here we build and benchmark models intended to **outperform GPT2** (124M) â€” all while keeping total training costs **under $10** (estimated GPU rental, late 2025). ğŸ’¸âš¡ï¸

- ğŸ§  **Cutting-edge Techniques:** Integrating the latest SOTA research to slash training time *and* expenses!
- ğŸ—ï¸ **Reference Point:** Standard GPT2 (124M params) as baseline, compared with improved models using:
    - ğŸ”¥ Smarter learning rate schedules
    - ğŸ¦ SwigLU activations
    - ğŸ”„ Rotary positional embeddings replacing absolute pos. embeddings

---

## ğŸ“ˆ Model Performance & Comparison

| ğŸš¦ Progress Over Baseline  |
|:-------------------------:|
| ![](media/convergence_comparison.png) |
| ![](media/commonsense_qa.png)         |

---

## ğŸ› ï¸ Training Models Made Easy

| ğŸ“š Model Variant           | â–¶ï¸ Command                      |
|:-------------------------- |:-------------------------------|
| ğŸŸ¦ GPT2 124M Baseline      | `make baseline`                 |
| ğŸŸ¥ Increased Max LR        | `make train_increase_max_lr`    |
| ğŸŸ© Rotational Pos Emb      | `make train_rotational_pos_emb` |

---

## ğŸ—ºï¸ Whatâ€™s Next?

- [x] âœ… commonsense_qa
- [x] âœ… exp 1 - baseline gpt
- [x] âœ… exp 2 - larger lr gpt2 until meet gpt2 perf
- [x] âœ… exp 3 - ROPE until meet gpt2 perf
- [ ] âœ³ï¸ exp 4 - Swiglu
- [x] âœ… gpt2 on commonsense_qa
- [ ] â« scale up the model
- [ ] ğŸ“Š v3 model performance on commonsense_qa 
- [ ] ğŸª¶ quantized inference and benchmark
- [ ] ğŸ§‘â€ğŸ’» vanilla C serve model

---

Letâ€™s make *state-of-the-art* cheap, fun, and open! ğŸŒŸ